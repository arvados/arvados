---
layout: default
navsection: installguide
title: Multi host Arvados
...
{% comment %}
Copyright (C) The Arvados Authors. All rights reserved.

SPDX-License-Identifier: CC-BY-SA-3.0
{% endcomment %}

# "Introduction":#introduction
# "Prerequisites and planning":#prerequisites
# "Download the installer":#download
# "Copy and customize the configuration files":#copy_config
# "Choose the SSL configuration":#certificates
## "Using a self-signed certificates":#self-signed
## "Using a Let's Encrypt certificates":#lets-encrypt
## "Bring your own certificates":#bring-your-own
# "Create a compute image":#create_a_compute_image
# "Further customization of the installation (modifying the salt pillars and states)":#further_customization
# "Installation order":#installation_order
# "Run the provision.sh script":#run_provision_script
# "Install the CA root certificate":#ca_root_certificate
# "Initial user and login":#initial_user
# "Test the installed cluster running a simple workflow":#test_install
# "After the installation":#post_install

h2(#introduction). Introduction

This multi host installer is an AWS specific example that is generally useful, but will likely need to be adapted for your environment. The installer is highly configurable.

h2(#prerequisites). Prerequisites and planning

Prerequisites:

* git
* a number of (virtual) machines for your Arvados cluster with at least 2 cores and 8 GiB of RAM, running a "supported Arvados distribution":{{site.baseurl}}/install/install-manual-prerequisites.html#supportedlinux
* a number of DNS hostnames that resolve to the IP addresses of your Arvados hosts
* ports 443 need to be reachable from your client (configurable in @local.params@, see below)
* port 80 needs to be reachable from everywhere on the internet (only when using "Let's Encrypt":#lets-encrypt without Route53 integration)
* SSL certificatse matching the hostnames in use (only when using "bring your own certificates":#bring-your-own)

Planning:

We suggest distributing the Arvados components in the following way, creating at least 6 hosts:

# Database server:
## postgresql server
# API node:
## arvados api server
## arvados controller
## arvados websocket
## arvados cloud dispatcher
## arvados keepbalance
# WORKBENCH node:
## arvados workbench
## arvados workbench2
## arvados webshell
# KEEPPROXY node:
## arvados keepproxy
## arvados keepweb
# KEEPSTORE nodes (at least 2)
## arvados keepstore
# SHELL node (optional):
## arvados shell

If your infrastructure differs from the setup proposed above (ie, using RDS or an existing DB server), remember that you will need to edit the configuration files for the scripts so they work with your infrastructure.

h2(#download). Download the installer

{% include 'download_installer' %}

h2(#copy_config). Copy and customize the configuration files

<notextile>
<pre><code>cp local.params.example.multiple_hosts local.params
cp -r config_examples/multi_host/aws local_config_dir
</code></pre>
</notextile>

Edit the variables in the <i>local.params</i> file. Pay attention to the <notextile><b>*_INT_IP, *_TOKEN</b> and <b>*_KEY</b></notextile> variables. The *SSL_MODE* variable is discussed in the next section.

{% include 'ssl_config_multi' %}

h2(#create_a_compute_image). Create a compute image

{% include 'branchname' %}

In a multi-host installation, containers are dispatched in docker daemons running in the <i>compute instances</i>, which need some special setup. We provide a "compute image builder script":https://github.com/arvados/arvados/tree/{{ branchname }}/tools/compute-images that you can use to build a template image following "these instructions":https://doc.arvados.org/install/crunch2-cloud/install-compute-node.html. Once you have that image created, you will need to update the <i>pillars/arvados.sls</i> file with the AMI ID and the private ssh key for the dispatcher.

h2(#further_customization). Further customization of the installation (modifying the salt pillars and states)

You will need further customization to suit your environment, which can be done editing the Saltstack pillars and states files. Pay particular attention to the <i>pillars/arvados.sls</i> file, where you will need to provide some information that describes your environment.

Any extra <i>state</i> file you add under <i>local_config_dir/states</i> will be added to the salt run and applied to the hosts.

h2(#installation_order). Installation order

A few Arvados nodes need to be installed in certain order. The required order is

* Database
* API server
* The other nodes can be installed in any order after the two above

h2(#run_provision_script). Run the provision.sh script

When you finished customizing the configuration, you are ready to copy the files to the hosts and run the @provision.sh@ script. The script allows you to specify the <i>role/s</i> a node will have and it will install only the Arvados components required for such role. The general format of the command is:

<notextile>
<pre><code>scp -r provision.sh local* user@host:
ssh user@host sudo ./provision.sh --roles comma,separated,list,of,roles,to,apply
</code></pre>
</notextile>

and wait for it to finish.

If everything goes OK, you'll get some final lines stating something like:

<notextile>
<pre><code>arvados: Succeeded: 109 (changed=9)
arvados: Failed:      0
</code></pre>
</notextile>

The distribution of role as described above can be applied running these commands:

h4. Database
<notextile>
<pre><code>scp -r provision.sh local* user@host:
ssh user@host sudo ./provision.sh --config local.params --roles database
</code></pre>
</notextile>

h4. API
<notextile>
<pre><code>scp -r provision.sh local* user@host:
ssh user@host sudo ./provision.sh --config local.params --roles api,controller,websocket,dispatcher,keepbalance
</code></pre>
</notextile>

h4. Keepstore(s)
<notextile>
<pre><code>scp -r provision.sh local* user@host:
ssh user@host sudo ./provision.sh --config local.params --roles keepstore
</code></pre>
</notextile>

h4. Workbench
<notextile>
<pre><code>scp -r provision.sh local* user@host:
ssh user@host sudo ./provision.sh --config local.params --roles workbench,workbench2,webshell
</code></pre>
</notextile>

h4. Keepproxy / Keepweb
<notextile>
<pre><code>scp -r provision.sh local* user@host:
ssh user@host sudo ./provision.sh --config local.params --roles keepproxy,keepweb
</code></pre>
</notextile>

h4. Shell (here we copy the CLI test workflow too)
<notextile>
<pre><code>scp -r provision.sh local* tests user@host:
ssh user@host sudo ./provision.sh --config local.params --roles shell
</code></pre>
</notextile>

{% include 'install_ca_cert' %}

h2(#initial_user). Initial user and login

At this point you should be able to log into the Arvados cluster. The initial URL will be:

* https://workbench.arva2.arv.local

or, in general, the url format will be:

* https://workbench.@<cluster>.<domain>@

By default, the provision script creates an initial user for testing purposes. This user is configured as administrator of the newly created cluster.

Assuming you didn't change these values in the @local.params@ file, the initial credentials are:

* User: 'admin'
* Password: 'password'
* Email: 'admin@arva2.arv.local'

h2(#test_install). Test the installed cluster running a simple workflow

If you followed the instructions above, the @provision.sh@ script saves a simple example test workflow in the @/tmp/cluster_tests@ directory in the @shell@ node. If you want to run it, just ssh to the node, change to that directory and run:

<notextile>
<pre><code>cd /tmp/cluster_tests
sudo /run-test.sh
</code></pre>
</notextile>

It will create a test user (by default, the same one as the admin user), upload a small workflow and run it. If everything goes OK, the output should similar to this (some output was shortened for clarity):

<notextile>
<pre><code>Creating Arvados Standard Docker Images project
Arvados project uuid is 'arva2-j7d0g-0prd8cjlk6kfl7y'
{
 ...
 "uuid":"arva2-o0j2j-n4zu4cak5iifq2a",
 "owner_uuid":"arva2-tpzed-000000000000000",
 ...
}
Uploading arvados/jobs' docker image to the project
2.1.1: Pulling from arvados/jobs
8559a31e96f4: Pulling fs layer
...
Status: Downloaded newer image for arvados/jobs:2.1.1
docker.io/arvados/jobs:2.1.1
2020-11-23 21:43:39 arvados.arv_put[32678] INFO: Creating new cache file at /home/vagrant/.cache/arvados/arv-put/c59256eda1829281424c80f588c7cc4d
2020-11-23 21:43:46 arvados.arv_put[32678] INFO: Collection saved as 'Docker image arvados jobs:2.1.1 sha256:0dd50'
arva2-4zz18-1u5pvbld7cvxuy2
Creating initial user ('admin')
Setting up user ('admin')
{
 "items":[
  {
   ...
   "owner_uuid":"arva2-tpzed-000000000000000",
   ...
   "uuid":"arva2-o0j2j-1ownrdne0ok9iox"
  },
  {
   ...
   "owner_uuid":"arva2-tpzed-000000000000000",
   ...
   "uuid":"arva2-o0j2j-1zbeyhcwxc1tvb7"
  },
  {
   ...
   "email":"admin@arva2.arv.local",
   ...
   "owner_uuid":"arva2-tpzed-000000000000000",
   ...
   "username":"admin",
   "uuid":"arva2-tpzed-3wrm93zmzpshrq2",
   ...
  }
 ],
 "kind":"arvados#HashList"
}
Activating user 'admin'
{
 ...
 "email":"admin@arva2.arv.local",
 ...
 "username":"admin",
 "uuid":"arva2-tpzed-3wrm93zmzpshrq2",
 ...
}
Running test CWL workflow
INFO /usr/bin/cwl-runner 2.1.1, arvados-python-client 2.1.1, cwltool 3.0.20200807132242
INFO Resolved 'hasher-workflow.cwl' to 'file:///tmp/cluster_tests/hasher-workflow.cwl'
...
INFO Using cluster arva2 (https://arva2.arv.local:8443/)
INFO Upload local files: "test.txt"
INFO Uploaded to ea34d971b71d5536b4f6b7d6c69dc7f6+50 (arva2-4zz18-c8uvwqdry4r8jao)
INFO Using collection cache size 256 MiB
INFO [container hasher-workflow.cwl] submitted container_request arva2-xvhdp-v1bkywd58gyocwm
INFO [container hasher-workflow.cwl] arva2-xvhdp-v1bkywd58gyocwm is Final
INFO Overall process status is success
INFO Final output collection d6c69a88147dde9d52a418d50ef788df+123
{
    "hasher_out": {
        "basename": "hasher3.md5sum.txt",
        "class": "File",
        "location": "keep:d6c69a88147dde9d52a418d50ef788df+123/hasher3.md5sum.txt",
        "size": 95
    }
}
INFO Final process status is success
</code></pre>
</notextile>

h2(#post_install). After the installation

Once the installation is complete, it is recommended to keep a copy of your local configuration files. Committing them to version control is a good idea.

Re-running the Salt-based installer is not recommended for maintaining and upgrading Arvados, please see "Maintenance and upgrading":{{site.baseurl}}/admin/maintenance-and-upgrading.html for more information.
